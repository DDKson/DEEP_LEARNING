{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u5SlhiyG1o9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9753d80-ab89-47bb-e572-27232ee92dcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
          ]
        }
      ],
      "source": [
        "import torch, torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import v2\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajhXsce1Uo5-"
      },
      "source": [
        "Cho 2 pretrained model trên 2 dataset CIFAR-10 và MNIST, finetune trên dataset mới FashionMNIST (có trong thư viện torchvision)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_state_dict = torch.load(\"/content/drive/MyDrive/DL4: Finetuning/cifar10_mini_vgg.pth\")\n",
        "mnist_state_dict = torch.load(\"/content/drive/MyDrive/DL4: Finetuning/mnist_mini_vgg.pth\")\n",
        "\n",
        "class MiniVGG(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "    ):\n",
        "        super(MiniVGG, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Linear(256 * 3 * 3, 10)\n",
        "        nn.init.normal_(self.classifier.weight, 0, 0.01)\n",
        "        nn.init.constant_(self.classifier.bias, 0)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "    ):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "model_cifar = MiniVGG().to(device)\n",
        "model_cifar.load_state_dict(cifar_state_dict, strict=False)\n",
        "model_mnist = MiniVGG().to(device)\n",
        "model_mnist.load_state_dict(mnist_state_dict, strict=False)\n",
        "\n",
        "model_cifar.features.requires_grad = False\n",
        "model_mnist.features.requires_grad = False"
      ],
      "metadata": {
        "id": "7i394lWVwds7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation: normalize ảnh từ scale [0, 255] về [-1,1], sử dụng ít nhất 2 trong số các transformations dưới đây cho dataset\n",
        "- Random resized\n",
        "- Center cropping\n",
        "- Random vertical flipping\n",
        "- Random horizontal flipping\n",
        "Các loại transformation khác có thể tham khảo ở đây\n"
      ],
      "metadata": {
        "id": "X9vJ0xrb57TT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-9WSyj5uVGEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f120d2-7c56-4d0b-9173-ff74ca525325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "bs = 64\n",
        "transform_train = v2.Compose([\n",
        "    v2.Grayscale(num_output_channels=1),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.Resize((28,28)),\n",
        "    v2.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = v2.Compose([\n",
        "    v2.Grayscale(num_output_channels=1),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.Resize((28,28)),\n",
        "    v2.ToTensor(),\n",
        "])\n",
        "\n",
        "fashionMNIST_train = FashionMNIST(root = \".\", train = True, download = True, transform=transform_train)\n",
        "fashionMNIST_test = FashionMNIST(root = \".\", train = False, download = True, transform=transform_test)\n",
        "fashionMNIST_train_loader = DataLoader(fashionMNIST_train, batch_size = bs, shuffle = True)\n",
        "fashionMNIST_test_loader = DataLoader(fashionMNIST_test, batch_size = 1, shuffle = True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "XR34a1_FQs_E",
        "outputId": "2997c8f3-202e-4abc-eb82-9c2dd7a4d245"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "image = fashionMNIST_train[20][0]\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YesTgVipcehf"
      },
      "outputs": [],
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  losses = []\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "      loss_train = 0.0\n",
        "      for imgs, labels in train_loader:\n",
        "          imgs = imgs.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = model(imgs)\n",
        "          loss = loss_fn(outputs, labels)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          loss_train += loss.item()\n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}'\n",
        "          .format(epoch, n_epochs, loss_train / len(train_loader)))\n",
        "      losses.append(loss_train / len(train_loader))\n",
        "\n",
        "  return losses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning: Cho mô hình MiniVGG như dưới, hãy load 2 pretrained models vào model MiniVGG ở dưới. Freeze self.features và train lớp cuối (self.classifier) của 2 pretrained models trên dataset FashionMNIST. Sau đó, train model MiniVGG from scratch trên tập FashionMNIST."
      ],
      "metadata": {
        "id": "ZrPxs1Hw6FjG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GHfnluvb0Ose",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc7de2c-b971-4572-a655-dfe071a03957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 1.0133\n",
            "Epoch [2/5], Loss: 0.4957\n",
            "Epoch [3/5], Loss: 0.4284\n",
            "Epoch [4/5], Loss: 0.3865\n",
            "Epoch [5/5], Loss: 0.3624\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "optimizer = torch.optim.SGD(model_cifar.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "losses_cifar = training_loop(\n",
        "    n_epochs = num_epochs,\n",
        "    optimizer = optimizer,\n",
        "    model = model_cifar,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = fashionMNIST_train_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ObN1V7OHskt",
        "outputId": "80a5f1eb-7d83-41dc-c933-32615dc8d597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 1.0879\n",
            "Epoch [2/5], Loss: 0.6331\n",
            "Epoch [3/5], Loss: 0.5353\n",
            "Epoch [4/5], Loss: 0.4849\n",
            "Epoch [5/5], Loss: 0.4528\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.SGD(model_mnist.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "losses_mnist = training_loop(\n",
        "    n_epochs = num_epochs,\n",
        "    optimizer = optimizer,\n",
        "    model = model_mnist,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = fashionMNIST_train_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ibJ-caeCxsnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3295fc9-2f03-4425-ad33-a5b2d235a8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 2.3022\n",
            "Epoch [2/5], Loss: 2.3013\n",
            "Epoch [3/5], Loss: 2.3000\n",
            "Epoch [4/5], Loss: 2.2978\n",
            "Epoch [5/5], Loss: 2.2928\n"
          ]
        }
      ],
      "source": [
        "VGG = MiniVGG().to(device)\n",
        "optimizer = torch.optim.SGD(VGG.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "losses_vgg = training_loop(\n",
        "    n_epochs = num_epochs,\n",
        "    optimizer = optimizer,\n",
        "    model = VGG,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = fashionMNIST_train_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Với 3 models, train 5 epochs và báo cáo accuracy trên tập test của 3 models"
      ],
      "metadata": {
        "id": "Venc2qQ66OU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "with torch.no_grad():\n",
        "    predictions = []\n",
        "    labels_l = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in fashionMNIST_test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model_cifar(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        predictions.append(predicted.item())\n",
        "        labels_l.append(labels.item())\n",
        "        del images, labels, outputs\n",
        "report_cifar = classification_report(predictions, labels_l)\n",
        "print(\"Classification report of fine tuned model from cifar\")\n",
        "print(report_cifar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-TD_ZHq8yd0",
        "outputId": "06b35728-3b01-43c4-b98b-5d3d61078b3a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report of fine tuned model from cifar\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.80      0.81      1028\n",
            "           1       0.96      0.98      0.97       975\n",
            "           2       0.81      0.79      0.80      1023\n",
            "           3       0.87      0.87      0.87      1008\n",
            "           4       0.83      0.76      0.79      1088\n",
            "           5       0.94      0.93      0.94      1010\n",
            "           6       0.60      0.68      0.64       879\n",
            "           7       0.91      0.92      0.92       988\n",
            "           8       0.95      0.96      0.96       987\n",
            "           9       0.95      0.93      0.94      1014\n",
            "\n",
            "    accuracy                           0.86     10000\n",
            "   macro avg       0.86      0.86      0.86     10000\n",
            "weighted avg       0.87      0.86      0.87     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    predictions = []\n",
        "    labels_l = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in fashionMNIST_test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model_mnist(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        predictions.append(predicted.item())\n",
        "        labels_l.append(labels.item())\n",
        "        del images, labels, outputs\n",
        "print(\"Classification report of fine tuned model from mnist\")\n",
        "report_mnist = classification_report(predictions, labels_l)\n",
        "print(report_mnist)"
      ],
      "metadata": {
        "id": "C9qSrIR180Yv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e81d32-4363-414e-c042-ef0edf5d9147"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report of fine tuned model from mnist\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.81      0.78       923\n",
            "           1       0.94      0.99      0.96       943\n",
            "           2       0.71      0.77      0.74       929\n",
            "           3       0.86      0.83      0.84      1043\n",
            "           4       0.79      0.72      0.75      1102\n",
            "           5       0.94      0.87      0.91      1077\n",
            "           6       0.63      0.61      0.62      1041\n",
            "           7       0.86      0.92      0.89       944\n",
            "           8       0.93      0.93      0.93      1001\n",
            "           9       0.92      0.93      0.93       997\n",
            "\n",
            "    accuracy                           0.83     10000\n",
            "   macro avg       0.83      0.84      0.84     10000\n",
            "weighted avg       0.83      0.83      0.83     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    predictions = []\n",
        "    labels_l = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in fashionMNIST_test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = VGG(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        predictions.append(predicted.item())\n",
        "        labels_l.append(labels.item())\n",
        "        del images, labels, outputs\n",
        "\n",
        "print(\"Classification report of model trained from scratch\")\n",
        "report_vgg = classification_report(predictions, labels_l)\n",
        "print(report_vgg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkE3Twfig0BA",
        "outputId": "03849ab9-6479-4c48-bed1-5a1558393a66"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report of model trained from scratch\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.50      0.01         8\n",
            "           1       0.00      0.00      0.00         0\n",
            "           2       0.08      0.06      0.07      1269\n",
            "           3       0.03      0.79      0.06        39\n",
            "           4       0.98      0.13      0.23      7551\n",
            "           5       0.00      0.00      0.00         0\n",
            "           6       0.00      0.00      0.00         0\n",
            "           7       0.00      0.00      0.00         0\n",
            "           8       0.03      0.03      0.03      1133\n",
            "           9       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.11     10000\n",
            "   macro avg       0.11      0.15      0.04     10000\n",
            "weighted avg       0.75      0.11      0.19     10000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  - Mô hình nào có accuracy cao nhất và tại sao?"
      ],
      "metadata": {
        "id": "gRthVoXBoayN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model cifar có kết quả cao nhất do dataset này gồm nhiều ảnh với chủ đề khác nhau nên có thể có nhiều feature tổng quát cho hình ảnh về thời trang, ngược lại dataset mnist chỉ là hình ảnh của những con số nên có thể không tốt bằng, model mini_VGG có kết quả tệ nhất vì số epoch chưa đủ để mô hình có thể học một cách hiệu quả."
      ],
      "metadata": {
        "id": "pNf2HUiom2Yq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### - Nếu train với nhiều epochs hơn thì accuracy của 3 models có giống nhau không?\n"
      ],
      "metadata": {
        "id": "N9Yo-ZluohX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nếu tăng số epochs lên đủ lớn thì model train từ đầu sẽ có kết quả tốt nhất so với 2 model finetune bởi những feature tạo ra ở model này fit với data hơn."
      ],
      "metadata": {
        "id": "4wuAaRreouE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature extractor: Với model tương tự như homework 2, dùng get_graph_node_names() và create_feature_extractor từ thư viện torchvision.models.feature_extraction để in ra tên layer và weight của layer tương ứng\n"
      ],
      "metadata": {
        "id": "Okvic8W9pekV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
        "import torch.nn.functional as F\n",
        "\n"
      ],
      "metadata": {
        "id": "OQMkxMIVmnjp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_nodes, eval_nodes = get_graph_node_names(model_mnist)"
      ],
      "metadata": {
        "id": "yXnWGyDmppao"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_feature_extractor(model_mnist, train_return_nodes = train_nodes, eval_return_nodes = eval_nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSBJiVGGpt2Z",
        "outputId": "db5ea64e-dee3-4f7f-fa7c-be7602836035"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MiniVGG(\n",
              "  (features): Module(\n",
              "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Linear(in_features=2304, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_mnist.features[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeplTES_5ZjW",
        "outputId": "b2530a16-c6ba-4dd4-9f4b-35767509fd8c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_mnist.features[5].weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnHovjnbwITy",
        "outputId": "a8902096-a8e2-491e-9839-7f60baa9b9c4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[-4.2842e-03, -2.1994e-02, -7.9081e-03],\n",
              "          [-3.1881e-02, -5.1875e-02, -2.2890e-02],\n",
              "          [-5.5099e-02, -4.3180e-02,  1.8146e-02]],\n",
              "\n",
              "         [[-9.0254e-03, -2.0297e-02, -2.3061e-02],\n",
              "          [ 5.2245e-02,  5.2892e-02,  4.6602e-02],\n",
              "          [ 1.5797e-03,  6.0276e-03, -6.3113e-03]],\n",
              "\n",
              "         [[-6.7427e-03, -3.4601e-03,  6.8334e-03],\n",
              "          [ 1.3255e-02,  3.7185e-02,  3.5690e-02],\n",
              "          [-1.6746e-02, -1.5730e-03,  1.3263e-02]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-1.1260e-02, -2.1871e-03, -1.1764e-02],\n",
              "          [-3.1190e-03, -5.6492e-03, -2.2009e-03],\n",
              "          [-1.3913e-02, -5.1290e-03,  3.4202e-03]],\n",
              "\n",
              "         [[-3.8904e-02, -3.9683e-02, -8.1803e-03],\n",
              "          [ 1.1483e-03,  3.3098e-03,  2.6540e-02],\n",
              "          [-3.1523e-02, -3.2937e-02,  8.1226e-03]],\n",
              "\n",
              "         [[-1.0552e-02,  1.1497e-02,  3.0405e-02],\n",
              "          [ 1.1333e-02,  4.5189e-02,  6.6236e-02],\n",
              "          [-2.5928e-02, -2.4763e-02, -1.0354e-02]]],\n",
              "\n",
              "\n",
              "        [[[-1.0260e-02, -2.3268e-02, -2.8499e-02],\n",
              "          [-3.4970e-02, -2.1051e-03, -1.0968e-02],\n",
              "          [-5.7697e-02, -3.2844e-02, -2.7342e-02]],\n",
              "\n",
              "         [[-4.1903e-03, -2.3563e-02,  2.0239e-03],\n",
              "          [-1.2113e-03,  4.4603e-02,  2.9250e-02],\n",
              "          [ 6.9668e-03,  7.0002e-02,  2.7061e-02]],\n",
              "\n",
              "         [[ 1.3881e-02,  1.1134e-02,  1.9045e-02],\n",
              "          [ 1.4120e-02,  2.4943e-02,  2.7081e-02],\n",
              "          [ 1.9003e-02,  1.8495e-02,  8.2616e-03]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 7.0985e-03, -2.4130e-03,  1.7556e-03],\n",
              "          [-1.0109e-02, -2.2274e-02, -1.3440e-02],\n",
              "          [-2.0628e-02, -2.5878e-02, -1.8062e-02]],\n",
              "\n",
              "         [[ 2.4562e-02,  3.3043e-03,  8.7847e-03],\n",
              "          [-2.5866e-02, -2.0535e-02, -4.7623e-04],\n",
              "          [-1.1898e-02, -9.2086e-03, -1.8159e-02]],\n",
              "\n",
              "         [[-1.8649e-02,  1.7182e-02,  2.4165e-02],\n",
              "          [ 7.7474e-03,  4.1719e-02,  4.9818e-02],\n",
              "          [ 6.9106e-02,  3.1646e-02,  4.1102e-02]]],\n",
              "\n",
              "\n",
              "        [[[ 1.9375e-02, -2.2686e-02, -2.0206e-02],\n",
              "          [-2.2638e-02, -2.5940e-02,  3.1525e-03],\n",
              "          [-3.1541e-02, -1.5306e-02,  1.9262e-02]],\n",
              "\n",
              "         [[ 2.1901e-02,  8.4009e-03, -1.3225e-02],\n",
              "          [ 2.4308e-02,  5.1358e-03,  3.8362e-02],\n",
              "          [-6.6355e-03,  1.3398e-02,  4.1561e-02]],\n",
              "\n",
              "         [[ 6.3306e-03, -2.5658e-03, -5.0412e-03],\n",
              "          [ 8.3865e-03,  1.8568e-02,  1.9831e-02],\n",
              "          [ 1.4890e-02,  2.9645e-02,  1.7989e-02]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-9.7051e-03, -1.2175e-02,  1.2131e-02],\n",
              "          [-5.6613e-03, -1.4877e-02,  3.1544e-03],\n",
              "          [ 4.6751e-03, -2.7689e-03, -1.7423e-02]],\n",
              "\n",
              "         [[-1.9543e-02, -4.9233e-03, -6.5896e-03],\n",
              "          [-1.5695e-02,  1.9970e-02,  8.2760e-03],\n",
              "          [ 4.9920e-03,  1.7756e-02,  8.0059e-03]],\n",
              "\n",
              "         [[-7.1162e-03,  1.6413e-02,  1.2720e-02],\n",
              "          [ 2.4863e-02,  6.6114e-02,  3.7366e-02],\n",
              "          [ 1.6521e-02,  2.4326e-02,  5.6678e-03]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[-5.7997e-41,  6.7391e-41, -2.1733e-41],\n",
              "          [ 3.4618e-41, -4.7358e-41, -8.6026e-42],\n",
              "          [ 4.0093e-41, -5.7439e-41, -4.2913e-41]],\n",
              "\n",
              "         [[-1.5459e-21, -6.1939e-23,  2.5992e-23],\n",
              "          [-1.0227e-21, -1.2627e-24, -2.1451e-25],\n",
              "          [-2.0031e-30, -2.3240e-37,  1.7002e-41]],\n",
              "\n",
              "         [[-2.1864e-23, -9.1708e-22,  2.7818e-23],\n",
              "          [-1.8903e-21, -1.2238e-20, -9.2604e-21],\n",
              "          [-2.1106e-27, -4.0725e-26,  1.0132e-26]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 6.2474e-15, -1.3297e-11, -2.8839e-12],\n",
              "          [-8.4879e-13, -8.0144e-12, -8.7581e-12],\n",
              "          [-5.9533e-13,  2.0936e-12, -4.6879e-12]],\n",
              "\n",
              "         [[-2.7118e-41, -8.1920e-42, -5.1781e-41],\n",
              "          [ 5.4679e-41,  5.0196e-41, -1.1807e-41],\n",
              "          [-4.7742e-42, -2.2946e-41,  2.9722e-41]],\n",
              "\n",
              "         [[ 2.9957e-41,  2.5760e-41,  5.5646e-42],\n",
              "          [ 5.9035e-41, -6.4754e-42,  3.7832e-41],\n",
              "          [-1.0741e-41,  3.9343e-41, -6.5092e-41]]],\n",
              "\n",
              "\n",
              "        [[[-4.8104e-03,  1.4769e-03, -1.1590e-02],\n",
              "          [ 5.0999e-03, -3.7735e-03, -2.2641e-02],\n",
              "          [-9.2431e-03, -7.4685e-03, -2.6542e-02]],\n",
              "\n",
              "         [[ 2.1916e-02,  1.4941e-02, -2.5095e-03],\n",
              "          [-2.1833e-02, -8.0815e-03, -7.5453e-03],\n",
              "          [-1.8552e-02, -2.2372e-02, -1.7234e-02]],\n",
              "\n",
              "         [[ 1.4542e-02,  1.1194e-02,  9.1559e-03],\n",
              "          [-2.2258e-03, -1.6545e-03, -3.2612e-03],\n",
              "          [-1.7253e-03, -3.7555e-03, -4.7920e-03]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-9.1777e-04, -7.7378e-03, -1.7728e-02],\n",
              "          [ 1.4604e-02,  1.6109e-02, -1.3094e-03],\n",
              "          [ 1.3205e-02, -3.0279e-03,  9.3736e-03]],\n",
              "\n",
              "         [[ 1.8060e-02,  1.1525e-02, -3.1073e-03],\n",
              "          [ 6.0209e-04, -1.3858e-03, -1.6885e-03],\n",
              "          [ 8.4708e-03,  7.2164e-04, -1.7926e-03]],\n",
              "\n",
              "         [[-7.6773e-03, -3.0904e-03,  6.2555e-03],\n",
              "          [-1.2732e-02, -1.0138e-02, -9.1385e-03],\n",
              "          [-1.0339e-03,  1.1812e-03, -2.3089e-03]]],\n",
              "\n",
              "\n",
              "        [[[-2.3824e-03, -1.3445e-03, -4.3938e-04],\n",
              "          [-2.5762e-03, -1.2082e-03, -2.8092e-04],\n",
              "          [-2.8183e-03, -8.9200e-04,  1.0924e-04]],\n",
              "\n",
              "         [[-1.2196e-03, -1.2759e-03, -1.2382e-03],\n",
              "          [-1.4493e-03, -1.2044e-03, -7.5492e-04],\n",
              "          [-2.3016e-03, -1.7212e-03, -3.6050e-04]],\n",
              "\n",
              "         [[-4.8474e-06,  8.0495e-06, -2.2096e-07],\n",
              "          [-1.7764e-06,  6.0821e-06, -5.6310e-08],\n",
              "          [-7.2132e-06, -1.4889e-05, -3.0685e-05]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-5.3792e-05, -2.4262e-05, -1.6177e-05],\n",
              "          [-7.7732e-05, -5.6619e-05, -4.4537e-05],\n",
              "          [-5.8435e-05, -9.0929e-05, -4.4454e-05]],\n",
              "\n",
              "         [[-4.3015e-04, -4.1075e-04, -1.8240e-04],\n",
              "          [-1.1530e-04, -1.5461e-04, -5.1963e-05],\n",
              "          [-4.5029e-05, -4.6573e-05, -3.3607e-05]],\n",
              "\n",
              "         [[-8.3888e-05, -4.4827e-05, -3.3152e-05],\n",
              "          [-5.6861e-05, -1.2785e-04, -5.3935e-05],\n",
              "          [-1.3051e-05, -1.2278e-04,  5.9866e-05]]]], device='cuda:0',\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}